
\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}
\usepackage[shortlabels]{enumitem}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}



%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf MLU Math Module
	\hfill #2} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Exercise #1: #4  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it  #3 \hfill } }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #4}{Lecture #1: #4}

   \vspace*{4mm}
}


%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newcommand{\trace}{\mbox{Trace}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**LECTURE-TITLE**}
\lecture{4}{May 2017}{}{Optimization methods}


% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Optimizing quadratics}
Let $f(\bu) = \frac{1}{2}(c - \bb^T \bu)^2$.
\subsection{Gradient}
The gradient of $f$ with respect to $\bu$ is given by:
\begin{enumerate}
\item $-2(c - \bb^T\bu)\bb $
\item $-2(c - \bb^T\bu)\bu$
\item $-(c - \bb^T\bu)\bb$
\item $-(c - \bb^T\bu)\bu$
\end{enumerate}


\subsection{Optimal solution}
Note that $f$ is actually a convex function in $\bu$ (thus it has only has global minima). The $\bu$ for which $f$ attains the minimum value is given by:
\begin{enumerate}
\item Any vector that is linearly independent with $\bb$
\item Any vector that is orthogonal to $\bb$
\item $\frac{\bb}{\|\bb\|_2^2}c$
\item $\mathbf{0}$
\end{enumerate}

\textbf{Fun fact:} Sometimes, even without having to take the gradient, the minimum solution becomes apparent. Can you see that why might be true
for the above problem? Hint: Is the function lower bounded below? Do we know the lower bound?

\section{Closest approximation of a vector}
Let $\bu, \bv$ be two vectors. Find the scalar $\alpha$ that makes $\alpha \bu$ as close as possible to $\bv$ in the euclidean distance.
That is, minimize $f(\alpha) = \|\alpha \bu - \bv\|_2^2$. 

\subsection{Gradient with respect to $\alpha$}
The gradient of $f$ with respect to $\alpha$ is given by:

\begin{enumerate}
\item $\alpha$
\item $2(\alpha \bu - \bv)$
\item $2(\alpha \bu - \bv)^T\bu$
\item $\bu^T(\alpha \bu - \bv)$
\end{enumerate}

\textbf{Hint: } The gradient with respect to a variable has to have the same dimension as the variable. Using this fact, you can rule out some of the possibilities.


\subsection{Closest approximation}
The scaling, $\alpha$ of $\bu$ that yields the closest vector to $\bv$ is given by:

\begin{enumerate}
\item $\frac{2\bu^T\bv}{\bu^T\bu}$
\item $\frac{\bu^T\bv}{\bu^T\bu}$
\item 1
\item $\bu^T\bv$
\end{enumerate}

\section{Closest approximation for a matrix}
Let's do the same thing as in the previous problem, but for matrices. Let $A,B$ be two matrices.
Let $\|.\|_F$ denote the Frobenius norm, which is essentially the square root of the sum of square of the entries of the matrix.
Find the $\alpha$ for which, $\|\alpha A - B\|_F^2$ is minimized.

\begin{enumerate}
\item $2\frac{\trace(A^TB)}{\|A\|_F^2}$
\item $\frac{\trace(A^TB)}{\|A\|_F^2}$
\item $\trace(A^TB)$
\item $\trace(A^TA)$
\end{enumerate}

\textbf{Hint; } Use the fact that $\|A\|_F^2 = \trace(A^TA)$
to expand the error function. Then, take the gradient of the function with respect to $\alpha$ and set it to zero.  Since the function is
convex in $\alpha$, we are guaranteed that the $\alpha$ so obtained is the global minimum.

\section{Ridge regression}
\begin{enumerate}
\item{
Let $f(\bx) =  \frac{1}{2}\|A\bx - \bb\|_2^2 + \lambda \|\bx\|_2^2$. $f$ is called the ridge regression function and is quite popular in ML.
The term $\lambda \|\bx\|_2^2$ is called the regularization function and ensures that we don't overfit! Find $\bx$ for which $f$ attains the minimum value?}
\end{enumerate}

\textbf{Hint: } Take the gradient of $f$ and set it to zero. Collect the terms containing $\bx$ and see if you can express
the solution as $C\bx = d$ for some matrix $C$ and vector $d$. Then $\bx = C^{-1}d$.

\section{Alternating optimization}
Alternating optimization shows up a lot in machine learning. k-means, a popular clustering algorithm is based on alternating optimization.
Matrix factorization is based on this (with an application to recommender systems). Even, EM algorithm uses alternating optimization. What is alternating optimization? If we have
a function of two variables - We fix one variable and minimize with respect to the other. And vice-versa. We repeat this process until both variables converge. \\

\begin{enumerate}
\item{
Let $f(\bu,\bv) = (c - \bu^T\bv)^2$. Find the gradient of $f$ with respect to $\bu$ at the point $(\bu^0, \bv^0)$. Let $\bu^1 = \bu^0 - s^0 \times \nabla_{\bu} f(\bu^0, \bv^0)$.
Next, find the gradient of $f$ with respect to $\bv$ at the point $(\bu^1, \bv^0)$. Let $\bv^1 = \bv^0 - s^1 \times \nabla_{\bv} f(\bu^1, \bv^0)$. \\}
\end{enumerate}

\textbf{Fun fact:} What we just did, was one iteration of alternating minimization of $f$. This is how algorithms are constructed - Compute gradients, take a step size in
the direction of the negative gradient and get the updated $\bu, \bv$. Repeat/iterate with the new updates. This process ensures that we are going towards
the local minima of $f$ and when the algorithm terminates, we would have found a $(\bu, \bv)$ that are closest to the local minima.


\section{Loss functions}

\begin{enumerate}
\item Consider the data set $\{(x_i, y_i)\}$, where $i=1,\ldots,n$. What is the square loss for linear regression?
\begin{enumerate}
\item $\displaystyle\sum_{i=1}^n(w^Tx_i+b-y_i)^2$
\item $\left(\displaystyle\sum_{i=1}^n(w^Tx_i+b-y_i)\right)^2$
\item $\left(\displaystyle\sum_{i=1}^n(w^Tx_i+b)\right)^2-\left(\displaystyle\sum_{i=1}^ny_i\right)^2$
\item $\displaystyle\sum_{i=1}^n(w^Tx_i+b)^2-\left(\displaystyle\sum_{i=1}^ny_i\right)^2$
\end{enumerate}

\end{enumerate}

\section{Gradient Descent}

\begin{enumerate}

\item Let $f(x,y)=x^2+xy-y^2$. What is the next step for the gradient descent at $(x,y)=(1,-1)$? Here $\alpha$ is the learning rate.

\item Let $w=\displaystyle\begin{bmatrix}w_1 \\ w_2\end{bmatrix}$ and $b=\displaystyle\begin{bmatrix}b_1 \\ b_2\end{bmatrix}$. Consider the loss function $L(w,b)=\|\displaystyle\begin{bmatrix}-1& 2\\-1& 4\end{bmatrix}w+b-\displaystyle\begin{bmatrix}1\\-1\end{bmatrix}\|^2$. What is the formula for the gradient descent at $w_1=w_2=b_1=b_2=1$? Here $\alpha$ is the learning rate.

\item {Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be the function given by $f(x) = x^4 - 3 x^3 + 2$. Let $x_0, x_1, x_2, \ldots$ be the sequence of values derived using gradient descent with step size 0.05 and initial value $x_0 = 1$. Using a tool like Excel, how many steps are needed for convergence?}
\begin{enumerate}
\item The sequences never converges.More than 20
\item 8
\item 5
\item 3
\end{enumerate}
 
\item {Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be the function given by $f(x,y) = x^2 - 2xy + 2y^2$. Let $P_0, P_1, P_2, \ldots$ be the sequence of points derived using gradient descent with step size 0.5 and initial point $P_0 = (4, 2)$. Determine $P_3$.}
\begin{enumerate}
 \item $(2, 1.25)$
 \item $(3, 2)$
 \item $(1, 1)$
 \item $(1, -1)$
\end{enumerate}

\end{enumerate}

\end{document}


